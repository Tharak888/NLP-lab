# -*- coding: utf-8 -*-
"""NLP TASK 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ikh3GOW8w2qACktzrvF0i1EyYMY5jkZt
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
nltk.download('punkt')
nltk.download('stopwords')
# Download the required punkt_tab resource
nltk.download('punkt_tab')
def tokenize_document(document):
    tokens = word_tokenize(document)
    return [word.lower() for word in tokens if word.isalpha()]
def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    return [word for word in tokens if word not in stop_words]
def find_morphology(tokens):
    fdist = FreqDist(tokens)
    return fdist.most_common(10)
# Document data provided as a string instead of from a file
document_text = """
This is an example document. It's used to demonstrate how to process text
without loading it from a file. We will analyze the words in this text,
tokenize them, remove common words like 'is' and 'a', and find the most
frequent words. The process is straightforward and helps us understand the
basic structure of the text.
"""
tokens = tokenize_document(document_text)
tokens_without_stopwords = remove_stopwords(tokens)
morphology = find_morphology(tokens_without_stopwords)
print("Morphology of the document:")
for word, frequency in morphology:
    print(f"{word}: {frequency}")