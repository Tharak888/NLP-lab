# -*- coding: utf-8 -*-
"""NLP LAB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fo1-t0tC22fU4YOf9PZz3En6RqFIS6JJ

TASK-1
"""

import re

pattern1 = re.compile(r'abc')
result1 = pattern1.match('abcdef')
if result1:
    print("Match found:", result1.group())

pattern2 = re.compile(r'.')
result2 = pattern2.search('Hello')
if result2:
    print("Character found:", result2.group())

pattern3 = re.compile(r'[aeiou]')
result3 = pattern3.search('Hello')
if result3:
    print("Vowel found:", result3.group())

pattern4 = re.compile(r'\d{3}-\d{2}-\d{4}')
result4 = pattern4.match('123-45-6789')
if result4:
    print("Social Security Number:", result4.group())

pattern5 = re.compile(r'\.')
result5 = pattern5.search('www.example.com')
if result5:
    print("Dot found:", result5.group())

pattern6 = re.compile(r'(\d{2})/(\d{2})/(\d{4})')
result6 = pattern6.match('01/09/2024')
if result6:
    print("Day:", result6.group(1))
    print("Month:", result6.group(2))
    print("Year:", result6.group(3))

pattern7 = re.compile(r'cat|dog')
result7 = pattern7.search('I have a cat and a dog')
if result7:
    print("Animal found:", result7.group())

"""TASK:2"""

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('punkt_tab')

text = "Tokenization without transformers is straightforward with tools like NLTK."
tokens = word_tokenize(text)
print("Tokens:", tokens)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens_transformers = tokenizer(text, return_tensors="pt")
print("Transformers Tokens:", tokens_transformers)

tokens_transformers_list = tokenizer.convert_ids_to_tokens(tokens_transformers['input_ids'][0].numpy().tolist())
print("Transformers Tokens (List):", tokens_transformers_list)

decoded_text = tokenizer.decode(tokens_transformers['input_ids'][0], skip_special_tokens=True)
print("Decoded Text:", decoded_text)

import numpy as np
import nltk
from nltk.tokenize import word_tokenize
import spacy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
nltk.download('punkt')
nltk.download('punkt_tab')
spacy.cli.download("en_core_web_sm")
nlp = spacy.load("en_core_web_sm")
corpus = "One disadvantage of using 'Best Of' samping is that it may lead to limited exploration of the model's knowledge and creativity. By focusing on the most probable next words, the model might generate responses that are safe and conventional, potentially missing out on more diverse and innovative outputs. The lack of exploration could result in repetitive or less imaginative responses, especially in situations where novel and unconventional ideas are desired.To address this limitation, other sampling strategies like temperature-based sampling or top-p (nucleus) sampling can be employed to introduce more randomness and encourage the model to explore a broader range of possibilities. However, it's essential to carefully balance exploration and exploitation based on the specific requirements of the task or application."
tokens = word_tokenize(corpus)
lemmatized_tokens = [token.lemma_ for token in nlp(corpus)]
all_tokens = tokens + lemmatized_tokens
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_tokens)
total_words = len(tokenizer.word_index) + 1
input_sequences = []
for line in all_tokens:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)
max_sequence_length = max(len(seq) for seq in input_sequences)
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')
X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = np.array(y)
model = Sequential()
model.add(Embedding(total_words, 100))
model.add(LSTM(100))
model.add(Dense(total_words, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=10, verbose=1)

"""TASK-3

"""

import numpy as np
import nltk
from nltk.tokenize import word_tokenize
import spacy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
nltk.download('punkt')
nltk.download('punkt_tab')
spacy.cli.download("en_core_web_sm")
nlp = spacy.load("en_core_web_sm")
corpus = "One disadvantage of using 'Best Of' samping is that it may lead to limited exploration of the model's knowledge and creativity. By focusing on the most probable next words, the model might generate responses that are safe and conventional, potentially missing out on more diverse and innovative outputs. The lack of exploration could result in repetitive or less imaginative responses, especially in situations where novel and unconventional ideas are desired.To address this limitation, other sampling strategies like temperature-based sampling or top-p (nucleus) sampling can be employed to introduce more randomness and encourage the model to explore a broader range of possibilities. However, it's essential to carefully balance exploration and exploitation based on the specific requirements of the task or application."
tokens = word_tokenize(corpus)
lemmatized_tokens = [token.lemma_ for token in nlp(corpus)]
all_tokens = tokens + lemmatized_tokens
tokenizer = Tokenizer()
tokenizer.fit_on_texts([corpus])
total_words = len(tokenizer.word_index) + 1

input_sequences = []
token_list = tokenizer.texts_to_sequences([corpus])[0]
for i in range(1, len(token_list)):
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)

max_sequence_length = max(len(seq) for seq in input_sequences)
input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')
X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = np.array(y)
model = Sequential()
model.add(Embedding(total_words, 100))
model.add(LSTM(100))
model.add(Dense(total_words, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=10, verbose=1)

"""TASK-4"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
nltk.download('punkt')
nltk.download('stopwords')
def tokenize_document(document):
    tokens = word_tokenize(document)
    return [word.lower() for word in tokens if word.isalpha()]
def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    return [word for word in tokens if word not in stop_words]
def find_morphology(tokens):
    fdist = FreqDist(tokens)
    return fdist.most_common(10)
# Document data provided as a string instead of from a file
document_text = """
This is an example document. It's used to demonstrate how to process text
without loading it from a file. We will analyze the words in this text,
tokenize them, remove common words like 'is' and 'a', and find the most
frequent words. The process is straightforward and helps us understand the
basic structure of the text. """

tokens = tokenize_document(document_text)
tokens_without_stopwords = remove_stopwords(tokens)
morphology = find_morphology(tokens_without_stopwords)
print("Morphology of the document:")
for word, frequency in morphology:
    print(f"{word}: {frequency}")

"""TASK-5

"""

import string
import random
import nltk
from nltk.corpus import stopwords, reuters
from collections import Counter, defaultdict
from nltk import FreqDist, ngrams
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('reuters')
nltk.download('punkt_tab') # Add this line to download the missing resource
sents = reuters.sents()
stop_word = set(stopwords.words('english'))
string.punctuation = string.punctuation + ' " ' + ' " ' + ' - ' + ' _ '
removal_list = list(stop_word) + list(string.punctuation) + ['\t', 'rt']
unigram = []
bigram = []
trigram = []
tokenized_text = []
for sentence in sents:
    sentence = list(map(lambda x: x.lower(), sentence))
    for word in sentence:
        if word == '.':
            sentence.remove(word)
        else:
            unigram.append(word)
        tokenized_text.append(word)
    bigram.extend(list(ngrams(sentence, 2, pad_left=True, pad_right=True)))
    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))
def remove_stopwords(x):
    y = []
    for pair in x:
        count = 0
        for word in pair:
            if word in removal_list:
                count = count or 0
            else:
                count = count or 1
        if count == 1:
            y.append(pair)
    return y
unigram = remove_stopwords(unigram)
bigram = remove_stopwords(bigram)
trigram = remove_stopwords(trigram)
freq_uni = FreqDist(unigram)
freq_bi = FreqDist(bigram)
freq_tri = FreqDist(trigram)
d = defaultdict(Counter)
for a, b, c in freq_tri:
    if (a is not None) and (b is not None) and (c is not None):
        d[a, b][c] += freq_tri[a, b, c]
def pick_word(counter):
    "choose a random element"
    return random.choice(list(counter.elements()))
prefix = "he", "is"
print(" ".join(prefix))
s = " ".join(prefix)
for i in range(19):
    suffix = pick_word(d[prefix])
    s = s + ' ' + suffix
    print(s)
    prefix = prefix[1], suffix

"""Task-6

"""

from nltk.util import ngrams
from nltk.lm import Laplace
from nltk.tokenize import word_tokenize
from nltk.lm.preprocessing import padded_everygram_pipeline

def ngram_smoothing(sentence, n):
    tokens = word_tokenize(sentence.lower())
    train_data, padded_sents = padded_everygram_pipeline(n, tokens)
    model = Laplace(n)
    model.fit(train_data, padded_sents)
    return model

sentence = input("Enter a sentence: ")
n = int(input("Enter the value of N for N-grams: "))
model = ngram_smoothing(sentence, n)
context = tuple(sentence.lower().split()[-n+1:])
next_words = model.generate(3, text_seed=context)
print("Next words:", ' '.join(next_words))

"""Task-7

"""

import nltk
nltk.download('all')
from nltk.tag import HiddenMarkovModelTagger
from nltk.corpus import treebank
nltk.download('treebank')
corpus=treebank.tagged_sents()
train_data=corpus[:3000]
test_data=corpus[3000:]
hmm_tagger=HiddenMarkovModelTagger.train(train_data)
sentence=input()
tokens=nltk.word_tokenize(sentence)
tagged_tokens=hmm_tagger.tag(tokens)
print(tagged_tokens)